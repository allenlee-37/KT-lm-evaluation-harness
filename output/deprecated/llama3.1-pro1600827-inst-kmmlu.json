{
  "results": {
    "kmmlu_direct_accounting": {
      "alias": "kmmlu_direct_accounting",
      "exact_match,none": 0.43,
      "exact_match_stderr,none": 0.049756985195624284
    },
    "kmmlu_direct_agricultural_sciences": {
      "alias": "kmmlu_direct_agricultural_sciences",
      "exact_match,none": 0.41,
      "exact_match_stderr,none": 0.01556091713692167
    },
    "kmmlu_direct_aviation_engineering_and_maintenance": {
      "alias": "kmmlu_direct_aviation_engineering_and_maintenance",
      "exact_match,none": 0.439,
      "exact_match_stderr,none": 0.015701131345400767
    },
    "kmmlu_direct_biology": {
      "alias": "kmmlu_direct_biology",
      "exact_match,none": 0.424,
      "exact_match_stderr,none": 0.015635487471405175
    },
    "kmmlu_direct_chemical_engineering": {
      "alias": "kmmlu_direct_chemical_engineering",
      "exact_match,none": 0.455,
      "exact_match_stderr,none": 0.015755101498347097
    },
    "kmmlu_direct_chemistry": {
      "alias": "kmmlu_direct_chemistry",
      "exact_match,none": 0.45,
      "exact_match_stderr,none": 0.020327042277376264
    },
    "kmmlu_direct_civil_engineering": {
      "alias": "kmmlu_direct_civil_engineering",
      "exact_match,none": 0.4,
      "exact_match_stderr,none": 0.015499685165842601
    },
    "kmmlu_direct_computer_science": {
      "alias": "kmmlu_direct_computer_science",
      "exact_match,none": 0.731,
      "exact_match_stderr,none": 0.014029819522568198
    },
    "kmmlu_direct_construction": {
      "alias": "kmmlu_direct_construction",
      "exact_match,none": 0.373,
      "exact_match_stderr,none": 0.015300493622922814
    },
    "kmmlu_direct_criminal_law": {
      "alias": "kmmlu_direct_criminal_law",
      "exact_match,none": 0.345,
      "exact_match_stderr,none": 0.03369796379336736
    },
    "kmmlu_direct_ecology": {
      "alias": "kmmlu_direct_ecology",
      "exact_match,none": 0.514,
      "exact_match_stderr,none": 0.015813097547730987
    },
    "kmmlu_direct_economics": {
      "alias": "kmmlu_direct_economics",
      "exact_match,none": 0.5076923076923077,
      "exact_match_stderr,none": 0.04401733523929784
    },
    "kmmlu_direct_education": {
      "alias": "kmmlu_direct_education",
      "exact_match,none": 0.54,
      "exact_match_stderr,none": 0.05009082659620333
    },
    "kmmlu_direct_electrical_engineering": {
      "alias": "kmmlu_direct_electrical_engineering",
      "exact_match,none": 0.39,
      "exact_match_stderr,none": 0.015431725053866611
    },
    "kmmlu_direct_electronics_engineering": {
      "alias": "kmmlu_direct_electronics_engineering",
      "exact_match,none": 0.554,
      "exact_match_stderr,none": 0.015726771166750357
    },
    "kmmlu_direct_energy_management": {
      "alias": "kmmlu_direct_energy_management",
      "exact_match,none": 0.341,
      "exact_match_stderr,none": 0.014998131348402718
    },
    "kmmlu_direct_environmental_science": {
      "alias": "kmmlu_direct_environmental_science",
      "exact_match,none": 0.36,
      "exact_match_stderr,none": 0.01518652793204012
    },
    "kmmlu_direct_fashion": {
      "alias": "kmmlu_direct_fashion",
      "exact_match,none": 0.476,
      "exact_match_stderr,none": 0.015801065586651758
    },
    "kmmlu_direct_food_processing": {
      "alias": "kmmlu_direct_food_processing",
      "exact_match,none": 0.44,
      "exact_match_stderr,none": 0.015704987954361805
    },
    "kmmlu_direct_gas_technology_and_engineering": {
      "alias": "kmmlu_direct_gas_technology_and_engineering",
      "exact_match,none": 0.398,
      "exact_match_stderr,none": 0.015486634102858929
    },
    "kmmlu_direct_geomatics": {
      "alias": "kmmlu_direct_geomatics",
      "exact_match,none": 0.438,
      "exact_match_stderr,none": 0.01569721001969469
    },
    "kmmlu_direct_health": {
      "alias": "kmmlu_direct_health",
      "exact_match,none": 0.64,
      "exact_match_stderr,none": 0.048241815132442176
    },
    "kmmlu_direct_industrial_engineer": {
      "alias": "kmmlu_direct_industrial_engineer",
      "exact_match,none": 0.478,
      "exact_match_stderr,none": 0.015803979428161936
    },
    "kmmlu_direct_information_technology": {
      "alias": "kmmlu_direct_information_technology",
      "exact_match,none": 0.711,
      "exact_match_stderr,none": 0.014341711358296188
    },
    "kmmlu_direct_interior_architecture_and_design": {
      "alias": "kmmlu_direct_interior_architecture_and_design",
      "exact_match,none": 0.623,
      "exact_match_stderr,none": 0.015333170125779854
    },
    "kmmlu_direct_korean_history": {
      "alias": "kmmlu_direct_korean_history",
      "exact_match,none": 0.37,
      "exact_match_stderr,none": 0.048523658709391
    },
    "kmmlu_direct_law": {
      "alias": "kmmlu_direct_law",
      "exact_match,none": 0.47,
      "exact_match_stderr,none": 0.015790799515836767
    },
    "kmmlu_direct_machine_design_and_manufacturing": {
      "alias": "kmmlu_direct_machine_design_and_manufacturing",
      "exact_match,none": 0.514,
      "exact_match_stderr,none": 0.01581309754773099
    },
    "kmmlu_direct_management": {
      "alias": "kmmlu_direct_management",
      "exact_match,none": 0.546,
      "exact_match_stderr,none": 0.015752210388771858
    },
    "kmmlu_direct_maritime_engineering": {
      "alias": "kmmlu_direct_maritime_engineering",
      "exact_match,none": 0.49,
      "exact_match_stderr,none": 0.020425359863232972
    },
    "kmmlu_direct_marketing": {
      "alias": "kmmlu_direct_marketing",
      "exact_match,none": 0.719,
      "exact_match_stderr,none": 0.014221154708434927
    },
    "kmmlu_direct_materials_engineering": {
      "alias": "kmmlu_direct_materials_engineering",
      "exact_match,none": 0.515,
      "exact_match_stderr,none": 0.01581217964181491
    },
    "kmmlu_direct_math": {
      "alias": "kmmlu_direct_math",
      "exact_match,none": 0.28,
      "exact_match_stderr,none": 0.025966276044877862
    },
    "kmmlu_direct_mechanical_engineering": {
      "alias": "kmmlu_direct_mechanical_engineering",
      "exact_match,none": 0.424,
      "exact_match_stderr,none": 0.01563548747140518
    },
    "kmmlu_direct_nondestructive_testing": {
      "alias": "kmmlu_direct_nondestructive_testing",
      "exact_match,none": 0.51,
      "exact_match_stderr,none": 0.01581613575277321
    },
    "kmmlu_direct_patent": {
      "alias": "kmmlu_direct_patent",
      "exact_match,none": 0.36,
      "exact_match_stderr,none": 0.04824181513244218
    },
    "kmmlu_direct_political_science_and_sociology": {
      "alias": "kmmlu_direct_political_science_and_sociology",
      "exact_match,none": 0.5266666666666666,
      "exact_match_stderr,none": 0.028874592695089598
    },
    "kmmlu_direct_psychology": {
      "alias": "kmmlu_direct_psychology",
      "exact_match,none": 0.459,
      "exact_match_stderr,none": 0.015766025737882158
    },
    "kmmlu_direct_public_safety": {
      "alias": "kmmlu_direct_public_safety",
      "exact_match,none": 0.406,
      "exact_match_stderr,none": 0.015537226438634599
    },
    "kmmlu_direct_railway_and_automotive_engineering": {
      "alias": "kmmlu_direct_railway_and_automotive_engineering",
      "exact_match,none": 0.379,
      "exact_match_stderr,none": 0.015349091002225345
    },
    "kmmlu_direct_real_estate": {
      "alias": "kmmlu_direct_real_estate",
      "exact_match,none": 0.415,
      "exact_match_stderr,none": 0.034928138718973496
    },
    "kmmlu_direct_refrigerating_machinery": {
      "alias": "kmmlu_direct_refrigerating_machinery",
      "exact_match,none": 0.396,
      "exact_match_stderr,none": 0.01547331326585941
    },
    "kmmlu_direct_social_welfare": {
      "alias": "kmmlu_direct_social_welfare",
      "exact_match,none": 0.539,
      "exact_match_stderr,none": 0.01577110420128319
    },
    "kmmlu_direct_taxation": {
      "alias": "kmmlu_direct_taxation",
      "exact_match,none": 0.38,
      "exact_match_stderr,none": 0.0344081327303582
    },
    "kmmlu_direct_telecommunications_and_wireless_technology": {
      "alias": "kmmlu_direct_telecommunications_and_wireless_technology",
      "exact_match,none": 0.625,
      "exact_match_stderr,none": 0.015316971293620996
    }
  },
  "group_subtasks": {
    "kmmlu_direct_electrical_engineering": [],
    "kmmlu_direct_aviation_engineering_and_maintenance": [],
    "kmmlu_direct_civil_engineering": [],
    "kmmlu_direct_public_safety": [],
    "kmmlu_direct_political_science_and_sociology": [],
    "kmmlu_direct_railway_and_automotive_engineering": [],
    "kmmlu_direct_interior_architecture_and_design": [],
    "kmmlu_direct_management": [],
    "kmmlu_direct_taxation": [],
    "kmmlu_direct_psychology": [],
    "kmmlu_direct_biology": [],
    "kmmlu_direct_marketing": [],
    "kmmlu_direct_math": [],
    "kmmlu_direct_geomatics": [],
    "kmmlu_direct_patent": [],
    "kmmlu_direct_health": [],
    "kmmlu_direct_gas_technology_and_engineering": [],
    "kmmlu_direct_food_processing": [],
    "kmmlu_direct_social_welfare": [],
    "kmmlu_direct_fashion": [],
    "kmmlu_direct_maritime_engineering": [],
    "kmmlu_direct_energy_management": [],
    "kmmlu_direct_ecology": [],
    "kmmlu_direct_korean_history": [],
    "kmmlu_direct_chemical_engineering": [],
    "kmmlu_direct_criminal_law": [],
    "kmmlu_direct_agricultural_sciences": [],
    "kmmlu_direct_telecommunications_and_wireless_technology": [],
    "kmmlu_direct_education": [],
    "kmmlu_direct_accounting": [],
    "kmmlu_direct_construction": [],
    "kmmlu_direct_computer_science": [],
    "kmmlu_direct_electronics_engineering": [],
    "kmmlu_direct_machine_design_and_manufacturing": [],
    "kmmlu_direct_materials_engineering": [],
    "kmmlu_direct_chemistry": [],
    "kmmlu_direct_nondestructive_testing": [],
    "kmmlu_direct_mechanical_engineering": [],
    "kmmlu_direct_real_estate": [],
    "kmmlu_direct_refrigerating_machinery": [],
    "kmmlu_direct_law": [],
    "kmmlu_direct_information_technology": [],
    "kmmlu_direct_environmental_science": [],
    "kmmlu_direct_economics": [],
    "kmmlu_direct_industrial_engineer": []
  },
  "configs": {
    "kmmlu_direct_accounting": {
      "task": "kmmlu_direct_accounting",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Accounting",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_agricultural_sciences": {
      "task": "kmmlu_direct_agricultural_sciences",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Agricultural-Sciences",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_aviation_engineering_and_maintenance": {
      "task": "kmmlu_direct_aviation_engineering_and_maintenance",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Aviation-Engineering-and-Maintenance",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_biology": {
      "task": "kmmlu_direct_biology",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_chemical_engineering": {
      "task": "kmmlu_direct_chemical_engineering",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Chemical-Engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_chemistry": {
      "task": "kmmlu_direct_chemistry",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_civil_engineering": {
      "task": "kmmlu_direct_civil_engineering",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Civil-Engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_computer_science": {
      "task": "kmmlu_direct_computer_science",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Computer-Science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_construction": {
      "task": "kmmlu_direct_construction",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Construction",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_criminal_law": {
      "task": "kmmlu_direct_criminal_law",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Criminal-Law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_ecology": {
      "task": "kmmlu_direct_ecology",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Ecology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_economics": {
      "task": "kmmlu_direct_economics",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Economics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_education": {
      "task": "kmmlu_direct_education",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Education",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_electrical_engineering": {
      "task": "kmmlu_direct_electrical_engineering",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Electrical-Engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_electronics_engineering": {
      "task": "kmmlu_direct_electronics_engineering",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Electronics-Engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_energy_management": {
      "task": "kmmlu_direct_energy_management",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Energy-Management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_environmental_science": {
      "task": "kmmlu_direct_environmental_science",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Environmental-Science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_fashion": {
      "task": "kmmlu_direct_fashion",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Fashion",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_food_processing": {
      "task": "kmmlu_direct_food_processing",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Food-Processing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_gas_technology_and_engineering": {
      "task": "kmmlu_direct_gas_technology_and_engineering",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Gas-Technology-and-Engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_geomatics": {
      "task": "kmmlu_direct_geomatics",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Geomatics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_health": {
      "task": "kmmlu_direct_health",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Health",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_industrial_engineer": {
      "task": "kmmlu_direct_industrial_engineer",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Industrial-Engineer",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_information_technology": {
      "task": "kmmlu_direct_information_technology",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Information-Technology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_interior_architecture_and_design": {
      "task": "kmmlu_direct_interior_architecture_and_design",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Interior-Architecture-and-Design",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_korean_history": {
      "task": "kmmlu_direct_korean_history",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Korean-History",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_law": {
      "task": "kmmlu_direct_law",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_machine_design_and_manufacturing": {
      "task": "kmmlu_direct_machine_design_and_manufacturing",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Machine-Design-and-Manufacturing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_management": {
      "task": "kmmlu_direct_management",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_maritime_engineering": {
      "task": "kmmlu_direct_maritime_engineering",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Maritime-Engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_marketing": {
      "task": "kmmlu_direct_marketing",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Marketing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_materials_engineering": {
      "task": "kmmlu_direct_materials_engineering",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Materials-Engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_math": {
      "task": "kmmlu_direct_math",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Math",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_mechanical_engineering": {
      "task": "kmmlu_direct_mechanical_engineering",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Mechanical-Engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_nondestructive_testing": {
      "task": "kmmlu_direct_nondestructive_testing",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Nondestructive-Testing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_patent": {
      "task": "kmmlu_direct_patent",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Patent",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_political_science_and_sociology": {
      "task": "kmmlu_direct_political_science_and_sociology",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Political-Science-and-Sociology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_psychology": {
      "task": "kmmlu_direct_psychology",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_public_safety": {
      "task": "kmmlu_direct_public_safety",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Public-Safety",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_railway_and_automotive_engineering": {
      "task": "kmmlu_direct_railway_and_automotive_engineering",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Railway-and-Automotive-Engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_real_estate": {
      "task": "kmmlu_direct_real_estate",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Real-Estate",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_refrigerating_machinery": {
      "task": "kmmlu_direct_refrigerating_machinery",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Refrigerating-Machinery",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_social_welfare": {
      "task": "kmmlu_direct_social_welfare",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Social-Welfare",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_taxation": {
      "task": "kmmlu_direct_taxation",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Taxation",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    },
    "kmmlu_direct_telecommunications_and_wireless_technology": {
      "task": "kmmlu_direct_telecommunications_and_wireless_technology",
      "tag": [
        "kmmlu",
        "kmmlu_direct"
      ],
      "dataset_path": "HAERAE-HUB/KMMLU",
      "dataset_name": "Telecommunications-and-Wireless-Technology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "{{question.strip()}}\nA. {{A}}\nB. {{B}}\nC. {{C}}\nD. {{D}}\n정답：",
      "doc_to_target": "{{['A', 'B', 'C', 'D'][answer-1]}}",
      "description": "",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true,
          "ignore_case": true,
          "ignore_punctuation": true,
          "regexes_to_ignore": [
            " "
          ]
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "Q:",
          "\n\n",
          "</s>",
          "."
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 2.0
      }
    }
  },
  "versions": {
    "kmmlu_direct_accounting": 2.0,
    "kmmlu_direct_agricultural_sciences": 2.0,
    "kmmlu_direct_aviation_engineering_and_maintenance": 2.0,
    "kmmlu_direct_biology": 2.0,
    "kmmlu_direct_chemical_engineering": 2.0,
    "kmmlu_direct_chemistry": 2.0,
    "kmmlu_direct_civil_engineering": 2.0,
    "kmmlu_direct_computer_science": 2.0,
    "kmmlu_direct_construction": 2.0,
    "kmmlu_direct_criminal_law": 2.0,
    "kmmlu_direct_ecology": 2.0,
    "kmmlu_direct_economics": 2.0,
    "kmmlu_direct_education": 2.0,
    "kmmlu_direct_electrical_engineering": 2.0,
    "kmmlu_direct_electronics_engineering": 2.0,
    "kmmlu_direct_energy_management": 2.0,
    "kmmlu_direct_environmental_science": 2.0,
    "kmmlu_direct_fashion": 2.0,
    "kmmlu_direct_food_processing": 2.0,
    "kmmlu_direct_gas_technology_and_engineering": 2.0,
    "kmmlu_direct_geomatics": 2.0,
    "kmmlu_direct_health": 2.0,
    "kmmlu_direct_industrial_engineer": 2.0,
    "kmmlu_direct_information_technology": 2.0,
    "kmmlu_direct_interior_architecture_and_design": 2.0,
    "kmmlu_direct_korean_history": 2.0,
    "kmmlu_direct_law": 2.0,
    "kmmlu_direct_machine_design_and_manufacturing": 2.0,
    "kmmlu_direct_management": 2.0,
    "kmmlu_direct_maritime_engineering": 2.0,
    "kmmlu_direct_marketing": 2.0,
    "kmmlu_direct_materials_engineering": 2.0,
    "kmmlu_direct_math": 2.0,
    "kmmlu_direct_mechanical_engineering": 2.0,
    "kmmlu_direct_nondestructive_testing": 2.0,
    "kmmlu_direct_patent": 2.0,
    "kmmlu_direct_political_science_and_sociology": 2.0,
    "kmmlu_direct_psychology": 2.0,
    "kmmlu_direct_public_safety": 2.0,
    "kmmlu_direct_railway_and_automotive_engineering": 2.0,
    "kmmlu_direct_real_estate": 2.0,
    "kmmlu_direct_refrigerating_machinery": 2.0,
    "kmmlu_direct_social_welfare": 2.0,
    "kmmlu_direct_taxation": 2.0,
    "kmmlu_direct_telecommunications_and_wireless_technology": 2.0
  },
  "n-shot": {
    "kmmlu_direct_accounting": 5,
    "kmmlu_direct_agricultural_sciences": 5,
    "kmmlu_direct_aviation_engineering_and_maintenance": 5,
    "kmmlu_direct_biology": 5,
    "kmmlu_direct_chemical_engineering": 5,
    "kmmlu_direct_chemistry": 5,
    "kmmlu_direct_civil_engineering": 5,
    "kmmlu_direct_computer_science": 5,
    "kmmlu_direct_construction": 5,
    "kmmlu_direct_criminal_law": 5,
    "kmmlu_direct_ecology": 5,
    "kmmlu_direct_economics": 5,
    "kmmlu_direct_education": 5,
    "kmmlu_direct_electrical_engineering": 5,
    "kmmlu_direct_electronics_engineering": 5,
    "kmmlu_direct_energy_management": 5,
    "kmmlu_direct_environmental_science": 5,
    "kmmlu_direct_fashion": 5,
    "kmmlu_direct_food_processing": 5,
    "kmmlu_direct_gas_technology_and_engineering": 5,
    "kmmlu_direct_geomatics": 5,
    "kmmlu_direct_health": 5,
    "kmmlu_direct_industrial_engineer": 5,
    "kmmlu_direct_information_technology": 5,
    "kmmlu_direct_interior_architecture_and_design": 5,
    "kmmlu_direct_korean_history": 5,
    "kmmlu_direct_law": 5,
    "kmmlu_direct_machine_design_and_manufacturing": 5,
    "kmmlu_direct_management": 5,
    "kmmlu_direct_maritime_engineering": 5,
    "kmmlu_direct_marketing": 5,
    "kmmlu_direct_materials_engineering": 5,
    "kmmlu_direct_math": 5,
    "kmmlu_direct_mechanical_engineering": 5,
    "kmmlu_direct_nondestructive_testing": 5,
    "kmmlu_direct_patent": 5,
    "kmmlu_direct_political_science_and_sociology": 5,
    "kmmlu_direct_psychology": 5,
    "kmmlu_direct_public_safety": 5,
    "kmmlu_direct_railway_and_automotive_engineering": 5,
    "kmmlu_direct_real_estate": 5,
    "kmmlu_direct_refrigerating_machinery": 5,
    "kmmlu_direct_social_welfare": 5,
    "kmmlu_direct_taxation": 5,
    "kmmlu_direct_telecommunications_and_wireless_technology": 5
  },
  "higher_is_better": {
    "kmmlu_direct_accounting": {
      "exact_match": true
    },
    "kmmlu_direct_agricultural_sciences": {
      "exact_match": true
    },
    "kmmlu_direct_aviation_engineering_and_maintenance": {
      "exact_match": true
    },
    "kmmlu_direct_biology": {
      "exact_match": true
    },
    "kmmlu_direct_chemical_engineering": {
      "exact_match": true
    },
    "kmmlu_direct_chemistry": {
      "exact_match": true
    },
    "kmmlu_direct_civil_engineering": {
      "exact_match": true
    },
    "kmmlu_direct_computer_science": {
      "exact_match": true
    },
    "kmmlu_direct_construction": {
      "exact_match": true
    },
    "kmmlu_direct_criminal_law": {
      "exact_match": true
    },
    "kmmlu_direct_ecology": {
      "exact_match": true
    },
    "kmmlu_direct_economics": {
      "exact_match": true
    },
    "kmmlu_direct_education": {
      "exact_match": true
    },
    "kmmlu_direct_electrical_engineering": {
      "exact_match": true
    },
    "kmmlu_direct_electronics_engineering": {
      "exact_match": true
    },
    "kmmlu_direct_energy_management": {
      "exact_match": true
    },
    "kmmlu_direct_environmental_science": {
      "exact_match": true
    },
    "kmmlu_direct_fashion": {
      "exact_match": true
    },
    "kmmlu_direct_food_processing": {
      "exact_match": true
    },
    "kmmlu_direct_gas_technology_and_engineering": {
      "exact_match": true
    },
    "kmmlu_direct_geomatics": {
      "exact_match": true
    },
    "kmmlu_direct_health": {
      "exact_match": true
    },
    "kmmlu_direct_industrial_engineer": {
      "exact_match": true
    },
    "kmmlu_direct_information_technology": {
      "exact_match": true
    },
    "kmmlu_direct_interior_architecture_and_design": {
      "exact_match": true
    },
    "kmmlu_direct_korean_history": {
      "exact_match": true
    },
    "kmmlu_direct_law": {
      "exact_match": true
    },
    "kmmlu_direct_machine_design_and_manufacturing": {
      "exact_match": true
    },
    "kmmlu_direct_management": {
      "exact_match": true
    },
    "kmmlu_direct_maritime_engineering": {
      "exact_match": true
    },
    "kmmlu_direct_marketing": {
      "exact_match": true
    },
    "kmmlu_direct_materials_engineering": {
      "exact_match": true
    },
    "kmmlu_direct_math": {
      "exact_match": true
    },
    "kmmlu_direct_mechanical_engineering": {
      "exact_match": true
    },
    "kmmlu_direct_nondestructive_testing": {
      "exact_match": true
    },
    "kmmlu_direct_patent": {
      "exact_match": true
    },
    "kmmlu_direct_political_science_and_sociology": {
      "exact_match": true
    },
    "kmmlu_direct_psychology": {
      "exact_match": true
    },
    "kmmlu_direct_public_safety": {
      "exact_match": true
    },
    "kmmlu_direct_railway_and_automotive_engineering": {
      "exact_match": true
    },
    "kmmlu_direct_real_estate": {
      "exact_match": true
    },
    "kmmlu_direct_refrigerating_machinery": {
      "exact_match": true
    },
    "kmmlu_direct_social_welfare": {
      "exact_match": true
    },
    "kmmlu_direct_taxation": {
      "exact_match": true
    },
    "kmmlu_direct_telecommunications_and_wireless_technology": {
      "exact_match": true
    }
  },
  "n-samples": {
    "kmmlu_direct_industrial_engineer": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_economics": {
      "original": 130,
      "effective": 130
    },
    "kmmlu_direct_environmental_science": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_information_technology": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_law": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_refrigerating_machinery": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_real_estate": {
      "original": 200,
      "effective": 200
    },
    "kmmlu_direct_mechanical_engineering": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_nondestructive_testing": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_chemistry": {
      "original": 600,
      "effective": 600
    },
    "kmmlu_direct_materials_engineering": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_machine_design_and_manufacturing": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_electronics_engineering": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_computer_science": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_construction": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_accounting": {
      "original": 100,
      "effective": 100
    },
    "kmmlu_direct_education": {
      "original": 100,
      "effective": 100
    },
    "kmmlu_direct_telecommunications_and_wireless_technology": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_agricultural_sciences": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_criminal_law": {
      "original": 200,
      "effective": 200
    },
    "kmmlu_direct_chemical_engineering": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_korean_history": {
      "original": 100,
      "effective": 100
    },
    "kmmlu_direct_ecology": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_energy_management": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_maritime_engineering": {
      "original": 600,
      "effective": 600
    },
    "kmmlu_direct_fashion": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_social_welfare": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_food_processing": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_gas_technology_and_engineering": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_health": {
      "original": 100,
      "effective": 100
    },
    "kmmlu_direct_patent": {
      "original": 100,
      "effective": 100
    },
    "kmmlu_direct_geomatics": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_math": {
      "original": 300,
      "effective": 300
    },
    "kmmlu_direct_marketing": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_biology": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_psychology": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_taxation": {
      "original": 200,
      "effective": 200
    },
    "kmmlu_direct_management": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_interior_architecture_and_design": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_railway_and_automotive_engineering": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_political_science_and_sociology": {
      "original": 300,
      "effective": 300
    },
    "kmmlu_direct_public_safety": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_civil_engineering": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_aviation_engineering_and_maintenance": {
      "original": 1000,
      "effective": 1000
    },
    "kmmlu_direct_electrical_engineering": {
      "original": 1000,
      "effective": 1000
    }
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=/vfroot/n22-model/f6e12a6705dc47f5af9f2b6bce90e0d4/93/40/e3564e474cf19dfb7e2fc364fe61/llama3.1-pro16-0827-Inst_241016",
    "model_num_parameters": 11520053248,
    "model_dtype": "torch.bfloat16",
    "model_revision": "main",
    "model_sha": "",
    "batch_size": "auto",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null,
    "random_seed": 0,
    "numpy_seed": 1234,
    "torch_seed": 1234,
    "fewshot_seed": 1234
  },
  "git_hash": "9b052fdc",
  "date": 1730795468.920959,
  "pretty_env_info": "PyTorch version: 2.4.0+cu121\nIs debug build: False\nCUDA used to build PyTorch: 12.1\nROCM used to build PyTorch: N/A\n\nOS: Ubuntu 22.04.2 LTS (x86_64)\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\nClang version: Could not collect\nCMake version: Could not collect\nLibc version: glibc-2.35\n\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\nPython platform: Linux-5.15.0-87-generic-x86_64-with-glibc2.35\nIs CUDA available: True\nCUDA runtime version: 12.2.128\nCUDA_MODULE_LOADING set to: LAZY\nGPU models and configuration: \nGPU 0: NVIDIA A100-SXM4-80GB\nGPU 1: NVIDIA A100-SXM4-80GB\nGPU 2: NVIDIA A100-SXM4-80GB\nGPU 3: NVIDIA A100-SXM4-80GB\nGPU 4: NVIDIA A100-SXM4-80GB\nGPU 5: NVIDIA A100-SXM4-80GB\nGPU 6: NVIDIA A100-SXM4-80GB\nGPU 7: NVIDIA A100-SXM4-80GB\n\nNvidia driver version: 535.104.12\ncuDNN version: Could not collect\nHIP runtime version: N/A\nMIOpen runtime version: N/A\nIs XNNPACK available: True\n\nCPU:\nArchitecture:                       x86_64\nCPU op-mode(s):                     32-bit, 64-bit\nAddress sizes:                      46 bits physical, 57 bits virtual\nByte Order:                         Little Endian\nCPU(s):                             192\nOn-line CPU(s) list:                0-191\nVendor ID:                          GenuineIntel\nModel name:                         Intel(R) Xeon(R) Platinum 8468\nCPU family:                         6\nModel:                              143\nThread(s) per core:                 2\nCore(s) per socket:                 48\nSocket(s):                          2\nStepping:                           8\nBogoMIPS:                           4200.00\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\nVirtualization:                     VT-x\nL1d cache:                          4.5 MiB (96 instances)\nL1i cache:                          3 MiB (96 instances)\nL2 cache:                           192 MiB (96 instances)\nL3 cache:                           210 MiB (2 instances)\nNUMA node(s):                       2\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191\nVulnerability Gather data sampling: Not affected\nVulnerability Itlb multihit:        Not affected\nVulnerability L1tf:                 Not affected\nVulnerability Mds:                  Not affected\nVulnerability Meltdown:             Not affected\nVulnerability Mmio stale data:      Not affected\nVulnerability Retbleed:             Not affected\nVulnerability Spec rstack overflow: Not affected\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\nVulnerability Srbds:                Not affected\nVulnerability Tsx async abort:      Not affected\n\nVersions of relevant libraries:\n[pip3] numpy==1.26.4\n[pip3] torch==2.4.0\n[pip3] torchvision==0.19.0\n[pip3] triton==3.0.0\n[conda] numpy                     1.26.4                   pypi_0    pypi\n[conda] torch                     2.4.0                    pypi_0    pypi\n[conda] torchvision               0.19.0                   pypi_0    pypi\n[conda] triton                    3.0.0                    pypi_0    pypi",
  "transformers_version": "4.45.2",
  "upper_git_hash": null,
  "tokenizer_pad_token": [
    "<|eot_id|>",
    "128009"
  ],
  "tokenizer_eos_token": [
    "<|eot_id|>",
    "128009"
  ],
  "tokenizer_bos_token": [
    "<|begin_of_text|>",
    "128000"
  ],
  "eot_token_id": 128009,
  "max_length": 131072,
  "task_hashes": {},
  "model_source": "hf",
  "model_name": "/vfroot/n22-model/f6e12a6705dc47f5af9f2b6bce90e0d4/93/40/e3564e474cf19dfb7e2fc364fe61/llama3.1-pro16-0827-Inst_241016",
  "model_name_sanitized": "__vfroot__n22-model__f6e12a6705dc47f5af9f2b6bce90e0d4__93__40__e3564e474cf19dfb7e2fc364fe61__llama3.1-pro16-0827-Inst_241016",
  "system_instruction": null,
  "system_instruction_sha": null,
  "fewshot_as_multiturn": false,
  "chat_template": null,
  "chat_template_sha": null,
  "start_time": 29520893.327746153,
  "end_time": 29525134.58278693,
  "total_evaluation_time_seconds": "4241.2550407759845"
}